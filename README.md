
# LuminaLib AI

AI-powered library backend system with:

* ğŸ“š Book upload (PDF)
* ğŸ¤– LLM-based summarization (Ollama)
* ğŸ§  Embedding generation
* ğŸ˜Š Sentiment analysis
* ğŸ¯ Embedding-based recommendation engine
* ğŸ— Clean Architecture design

---

## ğŸš€ Tech Stack

* Python 3.10+
* Flask
* PostgreSQL
* Ollama (local LLM)
* NumPy (cosine similarity)

---

## ğŸ— Project Structure

```
luminalib/
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ books.py
â”‚   â”œâ”€â”€ borrowings.py
â”‚   â”œâ”€â”€ reviews.py
â”‚
â”œâ”€â”€ application/
â”‚   â””â”€â”€ usecases/
â”‚       â”œâ”€â”€ add_book.py
â”‚       â”œâ”€â”€ borrow_book.py
â”‚       â”œâ”€â”€ add_review.py
â”‚       â”œâ”€â”€ recommend_books.py
â”‚       â”œâ”€â”€ summarize_book.py
â”‚
â”œâ”€â”€ domain/
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ recommendation_engine.py
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ connection.py
â”‚   â”‚   â””â”€â”€ repositories/
â”‚   â”‚       â”œâ”€â”€ book_repo.py
â”‚   â”‚       â”œâ”€â”€ borrowing_repo.py
â”‚   â”‚       â”œâ”€â”€ review_repo.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”œâ”€â”€ llm_base.py
â”‚   â”‚   â”œâ”€â”€ summary_service.py
â”‚   â”‚   â”œâ”€â”€ sentiment_service.py
â”‚   â”‚   â”œâ”€â”€ embedding_service.py
â”‚   â”‚   â”œâ”€â”€ pdf_reader.py
â”‚
â”œâ”€â”€ uploaded_books/
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
```

---

## ğŸ§  How Recommendation Works

1. Generate summary from PDF using Ollama
2. Generate embedding from summary
3. Store embedding in PostgreSQL (JSONB)
4. Build user profile embedding (average of liked books)
5. Compute cosine similarity
6. Return top recommended books

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone

```
git clone <repo-url>
cd luminalib
```

---

### 2ï¸âƒ£ Create Virtual Environment

```
python3 -m venv luminalib_venv
source luminalib_venv/bin/activate
```

---

### 3ï¸âƒ£ Install Dependencies

```
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Setup PostgreSQL

Create database and tables (see SQL schema in project).

---

### 5ï¸âƒ£ Install Ollama

```
ollama pull mistral
```

---

### 6ï¸âƒ£ Run Server

```
python main.py
```

Server runs at:

```
http://127.0.0.1:5000
```

---

## ğŸ§ª Example API Calls

### Upload Book

```
curl -X POST http://127.0.0.1:5000/books \
  -F "title=Deep Learning" \
  -F "author=Author" \
  -F "category=ML" \
  -F "description=Guide" \
  -F 'file=@/path/to/book.pdf'
```

---

### Borrow Book

```
curl -X POST http://127.0.0.1:5000/books/<BOOK_ID>/borrow
```

---

### Add Review

```
curl -X POST http://127.0.0.1:5000/books/<BOOK_ID>/reviews \
  -H "Content-Type: application/json" \
  -d '{"text":"Excellent book."}'
```

---

### Get Recommendations

```
curl http://127.0.0.1:5000/users/1/recommendations
```

---

## ğŸ“Œ Highlights

* Local LLM integration
* Semantic recommendation engine
* Clean architecture separation
* No external API dependency
* Fully self-hosted AI pipeline

---
