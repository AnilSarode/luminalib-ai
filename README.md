
# LuminaLib AI

AI-powered library backend system with:

* ğŸ“š Book upload (PDF)
* ğŸ¤– LLM-based summarization (Ollama)
* ğŸ§  Embedding generation
* ğŸ˜Š Sentiment analysis
* ğŸ¯ Embedding-based recommendation engine
* ğŸ— Clean Architecture design

---

## ğŸš€ Tech Stack

* Python 3.10+
* Flask
* PostgreSQL
* Ollama (local LLM)
* NumPy (cosine similarity)

---

## ğŸ— Project Structure

```
luminalib/
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ books.py
â”‚   â”œâ”€â”€ borrowings.py
â”‚   â”œâ”€â”€ reviews.py
â”‚
â”œâ”€â”€ application/
â”‚   â””â”€â”€ usecases/
â”‚       â”œâ”€â”€ add_book.py
â”‚       â”œâ”€â”€ borrow_book.py
â”‚       â”œâ”€â”€ add_review.py
â”‚       â”œâ”€â”€ recommend_books.py
â”‚       â”œâ”€â”€ summarize_book.py
â”‚
â”œâ”€â”€ domain/
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ recommendation_engine.py
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ connection.py
â”‚   â”‚   â””â”€â”€ repositories/
â”‚   â”‚       â”œâ”€â”€ book_repo.py
â”‚   â”‚       â”œâ”€â”€ borrowing_repo.py
â”‚   â”‚       â”œâ”€â”€ review_repo.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”œâ”€â”€ llm_base.py
â”‚   â”‚   â”œâ”€â”€ summary_service.py
â”‚   â”‚   â”œâ”€â”€ sentiment_service.py
â”‚   â”‚   â”œâ”€â”€ embedding_service.py
â”‚   â”‚   â”œâ”€â”€ pdf_reader.py
â”‚
â”œâ”€â”€ uploaded_books/
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
```

---

## ğŸ§  How Recommendation Works

1. Generate summary from PDF using Ollama
2. Generate embedding from summary
3. Store embedding in PostgreSQL (JSONB)
4. Build user profile embedding (average of liked books)
5. Compute cosine similarity
6. Return top recommended books

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone

```
git clone <repo-url>
cd luminalib
```

---

### 2ï¸âƒ£ Create Virtual Environment

```
python3 -m venv luminalib_venv
source luminalib_venv/bin/activate
```

---

### 3ï¸âƒ£ Install Dependencies

```
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Setup PostgreSQL Using Docker

This project uses PostgreSQL as the database.

The easiest way to run PostgreSQL is using Docker.

Your Flask app will run locally (outside Docker).
Only PostgreSQL will run inside a container.

4.1 Create docker-compose.yml

Create a file named:

docker-compose.yml

at the project root (luminalib/) and add the following content:

version: "3.9"

services:
  postgres:
    image: postgres:15
    container_name: luminalib_postgres
    restart: always
    environment:
      POSTGRES_USER: luminalib
      POSTGRES_PASSWORD: luminalib
      POSTGRES_DB: luminalib_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
4.2 Start PostgreSQL

From your project root:

docker-compose up -d

You should see something like:

Creating luminalib_postgres ... done
4.3 Verify PostgreSQL Is Running
4.3.1 Check Running Containers
docker ps

You should see:

luminalib_postgres
4.3.2 Connect to PostgreSQL (Recommended)
docker exec -it luminalib_postgres psql -U luminalib -d luminalib_db

If successful, you should see:

luminalib_db=#

To exit:

\q
4.4 Create Required Tables

After connecting to the database, run the following SQL:

CREATE TABLE books (
    id UUID PRIMARY KEY,
    title TEXT,
    author TEXT,
    category TEXT,
    description TEXT,
    file_path TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    summary TEXT,
    embedding JSONB
);

CREATE TABLE borrowings (
    id SERIAL PRIMARY KEY,
    book_id UUID REFERENCES books(id),
    user_id INT,
    borrowed_at TIMESTAMP,
    returned_at TIMESTAMP
);

CREATE TABLE reviews (
    id SERIAL PRIMARY KEY,
    book_id UUID REFERENCES books(id),
    user_id INT,
    text TEXT,
    sentiment TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);

4.5 Stop PostgreSQL

To stop the container:

docker-compose down

To stop and remove all database data:

docker-compose down -v
---

### 5ï¸âƒ£ Install Ollama

```
ollama pull mistral
```

---

### 6ï¸âƒ£ Run Server

```
python main.py
```

Server runs at:

```
http://127.0.0.1:5000
```

---

## ğŸ§ª Example API Calls

### Upload Book

```
curl -X POST http://127.0.0.1:5000/books \
  -F "title=Deep Learning" \
  -F "author=Author" \
  -F "category=ML" \
  -F "description=Guide" \
  -F 'file=@/path/to/book.pdf'
```

---

### Borrow Book

```
curl -X POST http://127.0.0.1:5000/books/<BOOK_ID>/borrow
```

---

### Add Review

```
curl -X POST http://127.0.0.1:5000/books/<BOOK_ID>/reviews \
  -H "Content-Type: application/json" \
  -d '{"text":"Excellent book."}'
```

---

### Get Recommendations

```
curl http://127.0.0.1:5000/users/1/recommendations
```

---

## ğŸ“Œ Highlights

* Local LLM integration
* Semantic recommendation engine
* Clean architecture separation
* No external API dependency
* Fully self-hosted AI pipeline

---
